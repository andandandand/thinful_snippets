{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boost guided example\n",
    "\n",
    "Having walked through gradient boost by hand, now let's try it with SKlearn.  We'll still use the European Social Survey Data, but now with a categorical outcome: Whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()\n",
    "\n",
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "\n",
    "# Put 90% of the data in the training set.\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8147, 16)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're now working with a binary outcome, we've switched to a classifier.  Now our loss function can't be the residuals.  Our options are \"deviance\", or \"exponential\".  Deviance is used for logistic regression, and we'll try that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04650845608292417\n",
      "Percent Type II errors: 0.17607746863066012\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06257668711656442\n",
      "Percent Type II errors: 0.18527607361963191\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "#what does ALL do\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7332"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_train.loc['All','All']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike decision trees, gradient boost solutions are not terribly easy to interpret on the surface.  But they aren't quite a black box.  We can get a measure of how important various features are by counting how many times a feature is used over the course of many decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAETCAYAAACobePkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcc2ec18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that age and happiness are the most important features in predicting whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DRILL: Improve this gradient boost model\n",
    "\n",
    "While this model is already doing alright, we've seen from the Type I and Type II error rates that there is definitely room for improvement.  Your task is to see how low you can get the error rates to go in the test set, based on your model in the training set.  Strategies you might use include:\n",
    "\n",
    "* Creating new features\n",
    "* Applying more overfitting-prevention strategies like subsampling\n",
    "* More iterations\n",
    "* Trying a different loss function\n",
    "* Changing the structure of the weak learner: Allowing more leaves in the tree, or other modifications\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Best Params to use with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_matrix = X.corr()\n",
    "# display(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(X.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations = df.corr().loc[:,'partner'].abs().sort_values(ascending=False)\n",
    "# correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-64e7e3d88293>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_search\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \"\"\"\n\u001b[1;32m--> 945\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[0;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m--> 564\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    606\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1026\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1028\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1029\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1082\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1083\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 787\u001b[1;33m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1030\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    348\u001b[0m                                            self.min_impurity_split)\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#adding to many param slows down run time, so I am going to select a few, and with those test out the other params\n",
    "#i might be interested in \n",
    "param_search = [{'n_estimators': [300, 400, 500],\n",
    "          'max_depth': [2,3,4],\n",
    "          'loss': ['deviance','exponential'],\n",
    "          'learning_rate': [0.05, .1, 0.2, 0.3],\n",
    "#           'min_samples_split': [10, 20],\n",
    "          'subsample':[.6, .8, 1]}]\n",
    "\n",
    "# Initialize and fit the model.\n",
    "grid_search = GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid=param_search, scoring = 'accuracy',cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-0229e3afd31d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mbest_params_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbest_params_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cv_results_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_index_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sbohan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# FIXME NotFittedError_ --> NotFittedError in 0.19\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0m_NotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print('\\n')\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, lets try using this params in our model. \n",
    "Question though. Do i need to fit again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04596290234588107\n",
      "Percent Type II errors: 0.18726132024004363\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.053987730061349694\n",
      "Percent Type II errors: 0.19754601226993865\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 300,\n",
    "          'max_depth':2,\n",
    "          'learning_rate': .05,\n",
    "          'loss': 'exponential',\n",
    "          'subsample':0.6}\n",
    "\n",
    "# Initialize and fit the model. #Did i already instantiate and fit though? \n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK this is not what I was expecting. It could be that I didn't add enough options in my param grid search. \n",
    "However, including more would take a much longer time. I am going to adjust one at a time to see if that works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.06833060556464812\n",
      "Percent Type II errors: 0.14634478996181124\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.12392638036809817\n",
      "Percent Type II errors: 0.15828220858895706\n"
     ]
    }
   ],
   "source": [
    "#learning rate changes\n",
    "params = {'n_estimators': 300,\n",
    "          'max_depth':2,\n",
    "          'learning_rate': 1,\n",
    "          'loss': 'exponential',\n",
    "          'subsample':0.6}\n",
    "\n",
    "# Initialize and fit the model. #Did i already instantiate and fit though? \n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.06560283687943262\n",
      "Percent Type II errors: 0.11074740861974905\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.13987730061349693\n",
      "Percent Type II errors: 0.15337423312883436\n"
     ]
    }
   ],
   "source": [
    "#learning rate of 1 appears to give the lowest accuracy\n",
    "#lets adjust the estimator size as well ---ends up that with a max_depth of 2, 300 works the best, lets adjust max_depth\n",
    "#learning rate changes\n",
    "params = {'n_estimators': 300,\n",
    "          'max_depth':3,\n",
    "          'learning_rate': 1,\n",
    "          'loss': 'exponential',\n",
    "          'subsample':0.6}\n",
    "\n",
    "# Initialize and fit the model. #Did i already instantiate and fit though? \n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.08210583742498637\n",
      "Percent Type II errors: 0.11756683033278778\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.150920245398773\n",
      "Percent Type II errors: 0.1558282208588957\n"
     ]
    }
   ],
   "source": [
    "#the difference in accuracy for train and test set grows, which might be a sign of overfitting\n",
    "# Let use 3 folds and change loss and play around with combos\n",
    "\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth':3,\n",
    "          'learning_rate': 1,\n",
    "          'loss': 'deviance',\n",
    "          'subsample':0.6}\n",
    "\n",
    "# Initialize and fit the model. #Did i already instantiate and fit though? \n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After playing around with the parameters, I have been able to improve the training set accuracies, and type Ii for my test (albeit very slightly), but at the sake of incresing type I error. I had tried different combinations in a previous workbook so lets take a look. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.02905073649754501\n",
      "Percent Type II errors: 0.13870703764320785\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.0785276073619632\n",
      "Percent Type II errors: 0.18527607361963191\n"
     ]
    }
   ],
   "source": [
    "#trying params that I had used in a prior version\n",
    "params = {'n_estimators': 450,\n",
    "          'max_depth':3,\n",
    "          'learning_rate': .2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model. #Did i already instantiate and fit though? \n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.02900107411385607\n",
      "Percent Type II errors: 0.13288322847936168\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.10736196319018405\n",
      "Percent Type II errors: 0.15828220858895706\n"
     ]
    }
   ],
   "source": [
    "#trying different holdout size \n",
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()\n",
    "\n",
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.8)\n",
    "\n",
    "# Put 90% of the data in the training set.\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, y_test = X[offset:], y[offset:]\n",
    "\n",
    "params = {'n_estimators': 450,\n",
    "          'max_depth':3,\n",
    "          'learning_rate': .2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model. #Did i already instantiate and fit though? \n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still while scarificing type i error, this is the lowest I have found without greatly incresing my type I error. \n",
    "The last thing I want to try is some feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "59px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
